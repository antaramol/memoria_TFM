%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for F. Javier Payán Somet at 2012-07-24 11:56:46 -0400 


%% Saved with string encoding Unicode (UTF-8) 

@article{KELLY2023101925,
	title = {What factors contribute to the acceptance of artificial intelligence? A systematic review},
	journal = {Telematics and Informatics},
	volume = {77},
	pages = {101925},
	year = {2023},
	issn = {0736-5853},
	doi = {https://doi.org/10.1016/j.tele.2022.101925},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585322001587},
	author = {Sage Kelly and Sherrie-Anne Kaye and Oscar Oviedo-Trespalacios},
	keywords = {AI, User acceptance, Psychosocial models, Human factors, Social robotics, Machine learning},
	abstract = {Artificial Intelligence (AI) agents are predicted to infiltrate most industries within the next decade, creating a personal, industrial, and social shift towards the new technology. As a result, there has been a surge of interest and research towards user acceptance of AI technology in recent years. However, the existing research appears dispersed and lacks systematic synthesis, limiting our understanding of user acceptance of AI technologies. To address this gap in the literature, we conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and meta-Analysis guidelines using five databases: EBSCO host, Embase, Inspec (Engineering Village host), Scopus, and Web of Science. Papers were required to focus on both user acceptance and AI technology. Acceptance was defined as the behavioural intention or willingness to use, buy, or try a good or service. A total of 7912 articles were identified in the database search. Sixty articles were included in the review. Most studies (n = 31) did not define AI in their papers, and 38 studies did not define AI for their participants. The extended Technology Acceptance Model (TAM) was the most frequently used theory to assess user acceptance of AI technologies. Perceived usefulness, performance expectancy, attitudes, trust, and effort expectancy significantly and positively predicted behavioural intention, willingness, and use behaviour of AI across multiple industries. However, in some cultural scenarios, it appears that the need for human contact cannot be replicated or replaced by AI, no matter the perceived usefulness or perceived ease of use. Given that most of the methodological approaches present in the literature have relied on self-reported data, further research using naturalistic methods is needed to validate the theoretical model/s that best predict the adoption of AI technologies.}
}



@electronic{VIU_article,
	author = {Universidad Internacional de Valencia},
	title = {¿Cuáles son las aplicaciones de la IA actuales y futuras?},
	year = {2023},
	month = {April},
	url = {https://www.universidadviu.com/es/actualidad/nuestros-expertos/cuales-son-las-aplicaciones-de-la-ia-actuales-y-futuras},
}



@electronic{Kaggle-dataset,
  author       = {Uldis Valainis},
  howpublished = {Kaggle},
  title        = {Audio Emotions},
  year         = {2020},
  url          = {https://www.kaggle.com/uldisvalainis/audio-emotions},
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{standfordCS224N,
	title={Understanding Emotion Classification In Audio Data},
	author={Gaurab Banerjee and Emily Huang and Allison Lettiere},
	project={CS224N: Natural Language Processing with Deep Learning},
	year={2021},
	Url={https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15785663.pdf}

}

@misc{sullivan2022improving,
      title={Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding}, 
      author={Peter Sullivan and Toshiko Shibano and Muhammad Abdul-Mageed},
      year={2022},
      eprint={2202.05209},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

	
@misc{baevski2020wav2vec,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning for Speech Recognition}, 
      author={Alexis Conneau and Alexei Baevski and Ronan Collobert and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.13979},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@electronic{greekEmotionRecognition,
  author       = {Mehrdad Farahani},
  howpublished = {GitHub},
  title        = {Emotion Recognition in Greek Speech Using Wav2Vec 2.0},
  year         = {2021},
  url          = {https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb},
}

% hugging face transformers webpage
@misc{transformers-docs,
	howpublished={Huggingface docs},
	title={Transformers},
	version={4.35.0},
	year={2021},
	url={https://huggingface.co/transformers/index.html},
}

% github project
@misc{voice-to-text-with-python-in-flask,
	author={Ruslan Magana Vsevolodovna},
	title={Voice to Text with Python in Flask},
	year={2021},
	url={https://github.com/ruslanmv/Voice-to-text-with-Python-in-Flask},
}

@article{AESDD_1,
author = {Vryzas, Nikolaos and Kotsakis, Rigas and Liatsou, Aikaterini and Dimoulas, Charalampos and Kalliris, George},
year = {2018},
month = {06},
pages = {457-467},
title = {Speech Emotion Recognition for Performance Interaction},
volume = {66},
journal = {Journal of the Audio Engineering Society. Audio Engineering Society},
doi = {10.17743/jaes.2018.0036}
}

@inproceedings{AESDD_2,
author = {Vryzas, Nikolaos and Matsiola, Maria and Kotsakis, Rigas and Dimoulas, Charalampos and Kalliris, George},
title = {Subjective Evaluation of a Speech Emotion Recognition Interaction Framework},
year = {2018},
isbn = {9781450366090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243274.3243294},
doi = {10.1145/3243274.3243294},
abstract = {In the current work, a conducted subjective evaluation of three basic components of a framework for applied Speech Emotion Recognition (SER) for theatrical performance and social media communication and interaction is presented. The multidisciplinary survey group used for the evaluation is consisted of participants with Theatrical and Performance Arts background, as well as Journalism and Mass Communications Studies. Initially, a publically available database of emotional speech utterances, Acted Emotional Speech Dynamic Database (AESDD) is evaluated. We examine the degree of agreement between the perceived emotion by the participants and the intended expressed emotion in the AESDD recordings. Furthermore, the participants are asked to choose between different coloured lighting of certain scenes captured on video. Correlations between the emotional content of the scenes and selected colors are observed and discussed. Finally, a prototype application for SER and multimodal speech emotion data gathering is evaluated in terms of Usefulness, Ease of Use, Ease of Learning and Satisfaction.},
booktitle = {Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion},
articleno = {34},
numpages = {7},
keywords = {automated lighting, subjective evaluation, emotional analysis},
location = {Wrexham, United Kingdom},
series = {AM '18}
}

@webpage{AESDD_webpage,
	author = {M3C: Multidisciplinary Media and Mediated Communication Research Group},
	title = {Acted Emotional Speech Dynamic Database (AESDD)},
	year = {2023},
	url = {http://m3c.web.auth.gr/research/aesdd-speech-emotion-recognition/}
}